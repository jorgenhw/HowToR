---
title: "HowToR"
author: "WIBE"
date: "9/30/2021"
output: html_document
---

# About this file
1. All headers are setup with correct notations which means, that you can open the document outline to get a quick overview of the notes. 
2. Beware that you can open/close headers/chunks for more structure; some headers might be closed.
3. For the purpose of getting a quick idea of some functions/plots, I've made a local df that isn't attached to a file: 
```{r}
var1 <- c(rnorm(200, 50, 20)) #Syntax: rnorm(n, mean, sd) (samples numbers from normal distribution)
var2 <- c(rnorm(200, 50, 20))
var3 <- c(rnorm(200, 50, 20))
var4 <- c(rnorm(200, 50, 20))

df <- data.frame(var1, var2, var3, var4)
```

# Basic statistical terms
*Standard deviation* is a measure of dispersion of the data from the mean.

    "Small standard deviations represented a scenario in which most data points were close to the mean, a large standard deviation represented a situation in which data points were widely spread from the mean" - Field
   
   
    
*Standard error of the mean* is a measure of how precise is our estimate of the mean. 

    "it is a measure of how representative a sample is likely to be of the population. A large standard error (relative to the sample mean) means that there is a lot of variability between the means of different samples and so the sample we have might not be representative of the population. A small standard error indicates that most sample means are similar to the population mean and so our sample is likely to be an accurate reflection of the population" - Field



*95% Confidence Interval*: when you see a 95% confidence interval for a mean, think of it like this: if we’d collected 100 samples, calculated the mean and then calculated a confidence interval for that mean then for
95 of these samples, the confidence intervals we constructed would contain the true value of the mean in the population.



# CONFIGURE SETUP CHUNK
```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)

# Load packages
library(pacman)
pacman::p_load(tidyverse,dplyr, data.table, vroom, ggplot2)

#how to get citations from packages
citation("examplePackage")
```

# LOAD DATA

## Create custom data
```{r}
# Generate random numbers from normal distribution: rnorm(n, mean, sd)
rnorm(100, 10, 2)
c(rnorm(100, 10, 2)) # 'c' indicates that R should interpret values as vector

# Create data set
dftest <- tibble(x = c(1,2,3), y = c(4,5,6))
rm(dftest)
```


## Single files
```{r}
# CSV files
df <- read.csv("filename from folder e.g. data.csv") #tidyverse: Use read_csv

# XLS (and other) files
library("readr")
readr::read_delim() # and specify (delim = "\t"), when doing so, we specify that as separator (change if file is comma seperated etc)
df <- read_delim("data.xls", delim= "\t")

dat <- read.delim("/Users/WIBE/Desktop/cogSci/Business Intelligence/7./adult.txt",sep = ",", header = TRUE, stringsAsFactors = TRUE)

```

## Multiple files
```{r}
library("vroom")
#1 create a data directory path pointing to data
datadir<-"/Users/rikkeuldbaek/Desktop/Cognitive Science/3rd semester/Perception & Action/Eye-tracking workshop/Data Cleaning/Search_Count_Task"

#2 create a list of specific files ending on "*.xls"
files_saccades <- list.files(datadir,pattern='*.xls',full.names=TRUE)

#merge all specific files into df
saccades <- vroom(files_saccades)

# COMBINE existing data sets by word
df2 <- df2 %>% inner_join(df1)

# R BIND - combines df's by row
newdf <- rbind(df1, df2)

# C BIND - combines df's by column
newdf <- cbind(df1, df2)

```

## Save files
```{r}
write.csv(data, file = "combined_logfiles.csv")
```


# VIEW DATA SET

## Overview of whole dataset
```{r}
# View variables, their classes etc
df %>% 
  str

# Basic statistical info of every variable
summary(df)

# Lists all variable names alphabetically
ls(df)
list(df) # Lists all variables, but in the order the occur in the df
```

## Overview of single variables
```{r}
# Basic info about variables
levels(df$var1)
class(df$var1)
unique(df$var1)

# Column names
colnames(df)
ls(df)

# Check mean, sd, median, var, skew, kurtosis of specific variables
round(pastecs::stat.desc(df$var1, basic = FALSE, norm = TRUE), digits = 2)
round(pastecs::stat.desc(cbind(Variable1 = df$var1, Variable2 = var2), basic = FALSE, norm = TRUE), digits = 2)

# Table especially useful for categorical variables. Or if you want to see how many TRUE or FALSE there are in variable. Or number of NA's. Etc. etc.
table(df$var1)
table(is.na(df$var1))

# VISUAL DISTRIBUTION quick view
hist(df$variable)

## histogram of something in 1 condition and another
hist(Saccades[Saccades$Task =="Search",]$Amplitude)# histogram of the amplitude in the search condition
hist(Saccades[Saccades$Task =="Count",]$Amplitude) #histogram of the amplitude in the count condition

```


# PREPROCESSING / TRANSFORMING

## Cleaning
```{r}
# DELETE a variable
df <- df %>% select(!var1)

# RENAME variable
rename(df, new_name = old_name)

# Rename values in variable
carsout$brand2[carsout$brand == " Europe."]  <-  "Europe"

# Clean variables- Keep only the variables needed and drop the rest! 
df <- df %>% 
  select(
    ParticipantID= "RECORDING_SESSION_LABEL", #new var-name = "current var-name"
    Trial= "TRIAL_INDEX",
    Time= "IP_START_TIME",
    LeftGazeX= "LEFT_GAZE_X",
    LeftGazeY= "LEFT_GAZE_Y",
    RightGazeX= "RIGHT_GAZE_X",
    RightGazeY= "RIGHT_GAZE_Y",
    LeftPupilSize= "LEFT_PUPIL_SIZE",
    RightPupilSize= "RIGHT_PUPIL_SIZE",
    Order,
    Task
    ) 

# Create new variable based on info from existing variables with an IF ELSE
df$var5 <- (ifelse(df$var1 <= 50, "under fifty","fifty or over fifty"))
# Nested ifelse statements, coool
df$var6 <- (ifelse(df$var1 < 50 & df$var1 > 40 ,"between 40 and 50",
                        ifelse(df$var1 < 40 | df$var1 > 50, "over 50 or under 40",NA))) # The NA will fill in every value that is not defined by the statements

# Remove unwanted punctuations etc from variables
df$stimulus <- gsub('[[:punct:] ]+','', df$stimulus)
sample$LeftGazeX <- as.numeric(gsub(",",".",sample$LeftGazeX))
gsub("current element we want to change", "substituting element", df$variable)

# GROUPBY() Making changes on portion of data 
dfnew <- df %>% 
  select(var1, var3, var5) %>% 
  group_by(var3) %>% 
  mutate(var3 = mean(var1))

df %>%
	group_by(Task) %>%
	summarise(mean_pupilsize = mean(PupilSize))

# FILTER()
Samples <-  Samples %>% 
  filter(
    PositionX<= 1680, #filter alt fra der er under eller lig med 1680
    PositionY<= 1050,#filter alt fra der er under eller lig med 1050
    PositionX>=0,#filter alt fra der er over eller lig med 0
    PositionY>=0#filter alt fra der er over eller lig med 0
  )

# SELECT() selects something specific, you can then do stuff with
select(variable, starts_with("a"))
select(iris, ends_with(".xls"))
select()
select(df,3:4)# Select 3rd and 4th columns of the dataframe
select(mydata,matches("di")) # Select on columns names of the dataframe which matches

# Assigning NA's to a X variables in a vector
df[df==0] <- NA
df$variable[df$variable=="."] <- NA #replace "." with NA

# Find outliers
## One way to do it, is to remove values that are more than three standard deviations away from the mean. This is easily done by z-standardizing values and then remove the values being more than three points away from mean on the z-scale.

#Identidy outliers:
# we want do identify cars below 900 pounds
which(carsout$weightlbs < 990)
carsout[c(228,258),]

```


## Transforming
Some data mining algorithms and statistical methods require that the variables be normally distributed.

Typical transforming methods are:
    1. *Log transformation*
        Good against positive skew or Unequal variances. Limmits: Can’t deal with negative numbers
    2. *Square root transformation*
        Good for positive skew or Unequal variances. Limits: Bigger effect than log(), still can’t deal with negative numbers
    3. *Reciprocal transformation*
        Good against positive skew - Unequal variances. Limits: Reduces large scores, good for negative numbers, but reverses scores
    4. *min/max normalization*
        Min-max normalization works by seeing how much greater the field value is than the minimum value min(X), and scaling this
        difference by the range
    5. *Z-score !standardization!* (not really a transformation tool, but standardization (good for detecting outliers))
        Very widespread in the world of statistical analysis. Works by taking the difference between the field value and the field mean
        value, and scaling this difference by the standard deviation of the field values. Z-score standardization has no effect on
        skewness
    6. *Inverse square root transformation*
    

Transforming in R              
```{r}
# sqrt() transformation
sqrt(df$var1)
# sqrt transforming multiple variables
for (i in 1:7){ #sqrt transforming variables 1-7
  varr <- sqrt(var(cars[,i]))
  meann <- mean(cars[,i])
  cars[,i] <- cars[,i]-meann/varr
}

# LOG transformation transformation
log(df$var1)

# Reciprocal transformation
1/df$var1

# Min/max transformation
## Doing it manually
df$var1_min.max <- (df$var1 - min(df$var1))/(max(df$var1) - min(df$var1))
## min/max normalization of all variables 1-3
  for (i in 1:3) {
  mindf = min(df[,i])
  maxdf = max(df[,i])
  df[,i] = (df[,i] - mindf)/(maxdf-mindf)
}

# Z-score transformation transformation
## doing z-transformation manually
cars$ztt60 <- (cars$time.to.60 - mean(cars$time.to.60))/sd(cars$time.to.60)
## Doing it using scale (which does the same as ^)
cars$ztscale <- scale(cars$time.to.60, center=TRUE, scale=TRUE)

# Inverse square root transformation
1 / sqrt(df$var1)

# Combining several transformations
df <- df %>% 
  mutate(rt_log = log(Reaction_Time),
         rt_sqrt = sqrt(Reaction_Time),
         rt_rec = 1/Reaction_Time)

```

## Normality check
Understanding the statistic of a Shapiro-Wilk test of normality (normtest.W) and its associated probability (normtest.p):
    *If the test is non-significant (p>.05) it tells us that the distribution of the sample is not significantly different from a normal distribution (i.e. it is NORMAL). If, however, the test is significant (p < .05) then the distribution in question is significantly different from a normal distribution (NOT NORMAL).*
```{r}
# stat.desc function outputs data which tells us if variables are breaking assumptions
round(pastecs::stat.desc(cbind(Variable1 = df$var1, sqrt_Variable1 = sqrt(var1), sqrt_Variable1 = sqrt(var1), sqrt_Variable1 = sqrt(var1)), basic = FALSE, norm = TRUE), digits = 2)
```


## Sanity check
```{r}
## Check distribution of raw samples. Is everything alright? Check it visually!!
hist(df$variable)

## numeric inspection
summary(df$variable)

## histogram of something in 1 condition and another
hist(Saccades[Saccades$Task =="Search",]$Amplitude)# histogram of the amplitude in the search condition
hist(Saccades[Saccades$Task =="Count",]$Amplitude) #histogram of the amplitude in the count condition
```


# CORRELATION TEST

To compute basic correlation coefficients there are three main functions that can be used: cor(), cor.test() and rcorr(). 

```{r}
#######Correlation coefficient:
cor(x,y, use = "string", method = "correlation type")

#x ix is a numeric variable or dataframe.
#y is another numeric variable
#use is set equal to a character string that specifies how missing values are handled. (1) “everything”, (2) “all.obs”, (3) “complete.obs”, (4) “pairwise. complete.obs” (read page 358 for further description)
#method enables you to specify whether you want “pearson”, “spearman” or “kendall” correlations, you can choose more than one. 

cor(df$balloon_balance, df$balloon, use = "everything", method = "pearson")


#For kun at printe selve værdien /// ved at kalde "estimate"
#Running Spearman correlation test: cor.test(x,y, method = 'spearman')
output_spearman <- cor.test(rdf$rt, rdf$word_length, method = 'spearman')

r_spearman <- output_spearman$estimate   

#writing down the estimate

#seeing output and result of r
output_spearman
r_spearman

#Calculating the R2 by taking the squareroot of r  (WAY Nr 1)
sqrt(r_spearman)
  

######The coefficient of determination r^2
#^2 means "squared"
cor(df$balloon_balance, df$balloon)^2



######Cor.test (non-parametric tests)
#Cor.test also calculates the p-value within the two variables

cor.test(df$balloon_balance, df$balloon, alternative = "less", method = "spearman")

#The "pearson" gives me a t-test aswell
cor.test(df$hours_music_per_week, df$sound_level_pref, alternative = "less", method = "pearson")

cor.test(df$balloon_balance, df$balloon, alternative = "less", method = "kendall")


######calculating the biserial correlation
install.packages("polycor")
library(polycor)
polyserial(df$hours_music_per_week, df$sound_level_pref)


##### Partial correlation:
install.packages("ggm")
library(ggm)

pcor(c("var1", "var2", "control1", "control2" etc.), var(dataframe))
#two first variables is for the partial correlation, the next two i for which you want to 'control' 

pcor(c("sound_level_pref", "hours_music_per_week", "balloon", "balloon_balance"), var(df))

#asigning it to a variable, to preform a squareroot
partial_correlation<- pcor(c("sound_level_pref", "hours_music_per_week", "balloon", "balloon_balance"), var(df))

partial_correlation^2

#to see the significance of the partial correlation, do a pcor.test
pcor.test(partial_correlation, 1, 44)

##### Highlight only the 'worrying' correlation values
#plot only values above .7
plot_df <- melt(cor(df))
plot_df$value[plot_df$value < 0.7 & plot_df$value > - 0.7] = 0
qplot(x=Var1, y=Var2, data=plot_df, fill=value, geom="tile") +
   scale_fill_gradient2(limits=c(-1, 1))

```

# BOOT STRAPPING
Bootstrapping is a statistical procedure that resamples a single dataset to create many simulated samples. 

This process allows you to calculate standard errors, construct confidence intervals, and perform hypothesis testing for numerous types of sample statistics. 

Bootstrap methods are alternative approaches to traditional hypothesis testing and are notable for being easier to understand and valid for more conditions.

```{r}

library(boot)

#The boot-function with correlation 
boot1 <- function(df,i) cor(df$sound_level_pref[i], df$hours_music_per_week[i], use = "complete.obs", method = "kendall")

#To create the bootstrap object:
boot_kendall <- boot(df, boot1, 2000)
boot_kendall

#95% confidentce interval for the boot_kendall object 
boot.ci(boot_kendall)

```



# VISUALIZING

## Combining base R plots
```{r}
par(mfrow=c(1,3))
qqnorm(resid(model4))
qqline(resid(model4))
qqnorm(resid(model_1)) # delete again
qqline(resid(model_1))
```


## Saving a ggplot
Saving ggplot (into your working directory)
```{r}
plot1 <- ggplot(df, aes(x=shoesize)) + geom_bar() #write a plot down
ggsave("myggplot.png")  # saves the last plot.
ggsave("mystoredggplot.png", plot=plot1)  # save a stored ggplot
```

## Correlation plots (heatmap)
```{r}
# a heatmap: Shows correlation visually
library(reshape2)
qplot(x=Var1, y=Var2, 
      data=melt(cor(df, use="p")), 
      fill=value, 
      geom="tile") +
   scale_fill_gradient2(limits=c(-1, 1))
```

## Histograms
Histograms can be used to inspect distributions in your data 
X-axis represents the different values in your variable 
Y-axis represents
  – counts (how many times does this value occur?)
  OR
  - density (what percentage of the total data has this value?) 
```{r}
#Histogram overlapping
data <- data.frame(
  a = df$balloon,
  b = df$balloon_balance
)

ggplot(data, aes(x=x) ) +
  geom_histogram( aes(x = a), fill = "green", alpha = 0.2) +
  geom_label( aes(x=4.5, y=0.25, label="Balloon"), color="green") +
  geom_histogram( aes(x = b), fill= "red", alpha = 0.2) +
  geom_label( aes(x=4.5, y=-0.25, label="Balloon Balance"), color="red")

#Histogram x2, comparing distribution

balloon_balance2 <- ggplot(df, aes(balloon_balance)) +
geom_histogram(aes(y = ..density..), colour = "black", fill = "white") + stat_function(fun = dnorm, args = list(mean = mean(df$balloon_balance),
sd = sd(df$balloon_balance)), colour = "red", size = 1) + theme_bw()

balloon2 <- ggplot(df, aes(balloon)) +
geom_histogram(aes(y = ..density..), colour = "black", fill = "white") + stat_function(fun = dnorm, args = list(mean = mean(df$balloon),
sd = sd(df$balloon)), colour = "red", size = 1) + theme_bw()

library(gridExtra)

grid.arrange(balloon2, balloon_balance2)
```

## Barplots
Beware that barplots has limitations; 
They be easily manipulated to yield false impressions, fail to reveal key assumptions, causes, effects, or patterns.
```{r}
#Basic barplot
ggplot(df, aes(x=gender, y=breathhold, fill=gender))+
geom_bar(stat = 'summary', fun = mean, width = 0.25, fill = 'green') +
  ggtitle("Bar plot")


#making bar plots with errorbars
audition %>% 
  ggplot(., aes(x=congruency, y=rt, fill=congruency))+
  geom_bar(stat = "summary", fun.y = mean)+
  geom_errorbar(stat = "summary", fun.data = mean_se, width = 0.2) +
  labs(x = "Congruency",
   y = "Reaction Time",
  title = "Plot 2.1: Mean RT based on congruency")+guides(fill=FALSE)
  
#E.g barplot with added "layers"
ggplot(df, aes(x=ocular_dom, y=breathhold, fill=ocular_dom))+
  geom_bar(stat='summary', fun.y =mean, width = 0.4)+
  geom_errorbar(stat = 'summary', fun.data = mean_se, width = 0.5)+
  labs(x = "Ocular Dominance", y = "Breathhold")+
  theme_minimal()+ ggtitle("Bar Plot") +
  scale_fill_brewer(palette = "Blues")
```

## Boxplots 
(much better than barplots)
```{r}
#Basic boxplot
ggplot(df, aes(x=gender, y=sound_level_pref))+
  geom_boxplot(width = 0.5) +
  ggtitle("Box Plot")

#E.g. boxplot with layers
ggplot(df, aes(x=gender, y=sound_level_pref , fill=gender))+
  geom_boxplot(width = 0.5) +
  ggtitle("Box Plot") +
  stat_summary(fun = mean, geom = "point", shape = 23, colour = "Black") + 
  geom_errorbar(stat = 'summary', fun.data = mean_se, width = 0.5)

#Box plot - changing the labels on the side
ggplot(df, aes(x = social_media, y = sleep_hours, colour = social_media)) +
  geom_boxplot(width = 0.5) +
  ggtitle("Hours of sleep by last social media used")+
  stat_summary(fun.y = mean, geom = "point", shape = 23, colour = "Black") +
  scale_color_manual(name="Social media",labels=c("Facebook", "Instagram", "No phone", "Other", "What's app"), values=c("red", "orange", "green", "dodgerblue", "maroon1")) +
  labs(x = "Social media", y = "Hours of sleep") +
  theme_minimal()+
  theme(axis.text.x= element_text(angle=90))#turning the angle 90 degrees
```

## Violinplot
```{r}
#Basic violin plot
  ggplot(subset(df,ocular_dom != "Both"), aes(x=ocular_dom, y=breathhold, fill= ocular_dom))+
geom_violin() +
  ggtitle("Violin Plot") 

#E.g. Violinplot with layers
ggplot(subset(df, ocular_dom != "Both"), aes(x=ocular_dom, y=breathhold, fill=ocular_dom))+
 geom_violin() +
  ggtitle("Violin Plot")+
  stat_summary(fun = mean, geom = "point", shape = 23, colour = "Black") 
```

##Scatterplots
```{r}
#Basic Scatterplot
ggplot(df, aes(x=tongue_twist, y=breathhold, colour = gender))+
  geom_point(geom = "point", shape = 19)+
  geom_smooth(method ="lm")

#E.g. Scatterplot with a few more layers
ggplot(df, aes(x=sound_level_pref, y=hours_music_per_week , fill=gender))+
  geom_point(geom = "point", shape = 23, colour = "red")+
  geom_smooth(method ="lm")

# method ="lm") makes the model linear (line straight) 
# without the above function is makes a curly line, that may vary from participant to participant

#same but without the confidence intervals around the line
ggplot(df, aes(sound_level_pref, hours_music_per_week)) + 
  geom_point() + 
  geom_smooth(method = lm, se = FALSE) + #fit the best straight line to the data, don't show confidence intervals
  ggtitle('Scatter plot with the regression line: geom_smooth without confidence intervals ')

#Combined scatterplot
ggplot(filter(data, ReactionTime < 10),aes(X1, ReactionTime, colour= Condition))+
  geom_point(position= "jitter")+
  geom_smooth(method="lm")+
  facet_wrap(~Condition, nrow =2)+
  geom_vline(xintercept = 58)+ #creates line on word 58
  geom_vline(xintercept= 59)+
  ggtitle("Reading Time pr Word - Condition 1 and 2 compared")
```

## QQ plots
What is it for?
Normal Q-Q Plot: This is used to assess if your residuals are normally distributed. basically what you are looking for here is the data points closely following the straight line at a 45% angle upwards (left to right). Again what to watch here is any patterns that deviate from this - particularly anything that looks curvilinear (bending at either end) or s shaped.

SKEWNESS
hvis den har en hale der varierer fra linjen, så er den enten right skewed (afviger i toppen af linjen) eller left skewed  (afviger i bunden af linjen).

QQ-plot: but what is a quantile: 
quantiles is just a generalization of median, quantile and percentile etc. 
(forestil dig en normal distbribution, delt på midten,  midten er MEDIAN)(50% på en ene side / 50% på den anden)
forestil dig en normaldistribution delt i fire, det er en quantile: 50% på den ene siden 50% på den anden, ud af hver 50% er de små områer 12,5 % (?)

```{r}
#Basic Q-Q plot
ggplot(df, aes(sample = balloon)) + stat_qq() +
stat_qq_line(colour = "red") +
labs(x = "Sample Quantiles", y = "Balloon") + ggtitle("Q-Q Plot") +
theme_bw()

#E.g. Q-Q plot 2x, comparing distribution, by assigning them to a variable and using the function grid.arrange

balloon <- ggplot(df, aes(sample = balloon)) + stat_qq() +
stat_qq_line(colour = "red") +
labs(x = "Sample Quantiles", y = "Balloon") + ggtitle("Q-Q Plot") +
theme_bw()

balloon_balance <- ggplot(df, aes(sample = balloon_balance)) + stat_qq() +
stat_qq_line(colour = "red") +
labs(x = "Sample Quantiles", y = "Balloon Balance") + ggtitle("Q-Q Plot") +
theme_bw()

grid.arrange(balloon, balloon_balance)

# Tests the emperical, z-scored data against the theoretical, normally distributed data using qqnorm(data$column)
## #qq-plot for residuals (normal qq plot for residuals)
qqnorm(resid(model1))
qqline(resid(model1))
```

## Density plot
```{r}
ggplot(politeness, aes(f0mn))+geom_density()
```

## Plot interaction effect
```{r}
plot(allEffects(model6), multiline=TRUE, ci.style="bars")
```


# T-TEST

T-test is the comparison of two means (simply)
We have 3 t-tests: 

## Independent t-test (welch or Student's)
```{r}
#PARAMETRIC TESTS (if normally distributed)

##### Welch t-test 
#2.1 Independent Welch t-test (default): t.test(Measure ~ Group, data = dataFrame/tibble)
#Performs Welch Two Sample t-test, which is an independent (unpaired) t-test. It requires your data to be normally distributed in both groups and allows variances in these groups to be different.
t.test(ReactionTime ~ Condition, data = data)


#####Students t-test
#change 'var.equal' argument to True to perform a student's t-test, rather than the default Welch's
#2.2 Independent Student's t-test: t.test(Measure ~ Group, data = , var.equal = T)
#'var.equal' argument is a logical variable indicating whether to treat the two variances as being equal. When set to true, your variances are assumed to be equal in two groups, and test becomes a Student's t-test. It assumes that the two populations have normal distributions with equal variances. 
t.test(ReactionTime ~ Condition, data = data, var.equal = T) 
```

## Paired t-test
```{r}
# ALSO PARAMETRIC (for normally distributed data)
#2.3 A paired t-test: t.test(Measure ~ Group, data = , paired = T)
#'paired' argument indicates whether you want a paired t-test (aka repeated measures: both group 1 and group 2 consist of the same participants ). It's meaningless in the context of our study, but you can try to run it anyway. 
#set 'paired' argument to True to perform a paired (dependent) t-test (might not work due to our experimental design)

t.test(ReactionTime ~ Condition, data = data, paired = T)
```

## One-sample-t-test
```{r}
#2.4 A one sample t-test: t.test(df$Measure, mu = )
#mu is a number indicating the true value of the mean. One-sample t-test is used to compare the mean of one sample to a known standard (or theoretical/hypothetical) mean (mu). Generally, the theoretical mean comes from either a previous experiment or from specifics of your experimental design. #a one sample t-test: is mean of our sample different from the theoretical mean of 0.5

t.test(data$ReactionTime, mu = 0.5)  
```


## Non-parametric t-test 
If data is NOT normally distributed
```{R}
#  2.5.1  Independent t-test: WRS2::yuen(Measure ~ Group, data = data)
WRS2::yuen(ReactionTime ~ Gender, data = data)

#2.5.2 Paired t-test: WRS2::yuen(x, y, tr = 0.2)
WRS2::yuen(x, y, tr = 0.2)



#REMEMBER TO VISUALIZE YOUR RESULTS
#Jonathan's example from class 6 
transformed_data$ID <- as.factor(transformed_data$ID)
ggplot(transformed_data, aes(x = ID, y = rt_rec, colour = ID)) +
  theme_minimal() +
  labs(x = "Condition", y = "Reading time (reciprocal transform)") +
  geom_boxplot(width = 0.5) +
  ggtitle("Box Plot: reciprocally transformed reading time depending on condition")

```

## Reporting t-test
Example: *Using an independent t-test, we found that the unexpected word did not significantly increase the average reading time of a word, t(7.66) = 0.46, p > .05, r = 0.16, (M exp = 0.45, M unexp = 0.49)*

# MODELLING

## Assumptions of regression

*Multicollinearity*
!!!**basically ik put noget i din model der correlater med hinanden, så som tømmermænd og forelæsning, begge dele gør dig træt**!!!!

However, you should still keep in mind that if you add two predictors that are highly correlated, the estimates of your regression will become hard to interpret and unstable. 
Intuitively, this makes a lot of sense: If multiple predictors are very similar to each other, then it becomes very difficult to decide what, in fact, is playing a big role.

Therefore, make sure you always check correlation between your predictors!

You can check how all of variables in your data are correlated with each other by running cor() function on the entire dataset. High correlation coefficients (e.g. over 0.7) are worrying and indicate that it might be better not to include such correlated predictors in the same model together. To make it easier to see, round up the results to 2 decimal points using round() function:

OR make heatmap 
You can make it even easier by displaying the correlation matrix of all predictors as a heatmap.

From the author of this code:
"*What is going on?*
- cor makes a correlation matrix with all the pairwise correlations between variables (twice; plus a diagonal of ones)

- melt takes the matrix and creates a data frame in long form, each row consisting of id variables Var1 and Var2 and a single value

- We then plot with the tile geometry, mapping the indicator variables to rows and columns, and value (i.e. correlations) to the fill colour.

- In ggplot2, a scale that has a midpoint and a different colour in each direction is called scale_colour_gradient2, and we just need to add it. I also set the limits to -1 and 1, which doesn’t change the colour but fills out the legend for completeness. Done!"

... After you've examined this, you just need to make sure that you don't put highly correlated predictors in the same model at the same time. In this case, it seems that Television and Parenting style variables are correlated more than others. Even though their correlation coefficient is below 0.7 (it's 0.53), you still might want to avoid putting those two together as predictors in a model.
```{r}
#1 check cor()
cor(variable)
round(cor(variable),digits= 2) #much better! but still hard...


#2 heat map
pacman::p_load(ggplot2, reshape2)

#plot as heatmap
qplot(x=Var1, y=Var2, 
      data=melt(cor(df, use="p")), 
      fill=value, 
      geom="tile") +
   scale_fill_gradient2(limits=c(-1, 1))



##### Highlight only the 'worrying' correlation values
#plot only values above .7
plot_df <- melt(cor(df))
plot_df$value[plot_df$value < 0.7 & plot_df$value > - 0.7] = 0
qplot(x=Var1, y=Var2, data=plot_df, fill=value, geom="tile") +
   scale_fill_gradient2(limits=c(-1, 1))


```


*Linearity* 
It’s called “linear model” for a reason! 
If data is NOT linear, a residual plot will indicate some kind of curve, or it will indicate some other pattern (e.g., two lines if you have categorical binary data).
  -no patterns or curves  PLEASE!!!
```{r}
#residual plot 
plot(model)

```
This is a residual plot. The fitted values (the predicted means) are on the horizontal line (at y=0). The residuals are the vertical deviations from this line. This view is just a rotation of the actual data (compare the residual plot with the scatterplot to see this).


*Homoschedasticity*
It says that the variability of your data should be approximately equal across the range of your predicted values (we want equal variances). If homoscedasticity is violated, you end up with heteroskedasticity, or, in other words, a problem with unequal variances.
- look at the plot "residuals VS fitted"
    - WE WANT A CLOUD OF DOTS!

```{r}
#make residual plot
plot(model)

#ideal plot with homoschedasticity looks like this
plot(rnorm(100),rnorm(100))

```

*Normality of residuals*
.. the least important.. it turns out that linear models are relatively robust against violations of the assumptions of normality. 
.. according to LAU this is a subject matter.. 

How to check it? 
- histograms and qq-plot of model
```{r}
#histogram of model
hist(residuals(model)) #bell shaped =normal

# Q-Q plot of model
qqnorm(residuals(model)) #dots fall in a line = normal


```


*Absence of influential data points* 
not really an assumption mby, but influential data points can drastically change the interpretation of your results, and similar to collinearity, it can lead to instable results.

How to check it? 
```{r}
#this function:
dfbeta(model)

summary(model)
```

For each coefficient of your model (including the intercept), the function gives you the so-called DFbeta values. These are the values with which the coefficients have to be adjusted if a particular data point is excluded (sometimes called “leave-one-out diagnostics”).

[E.g] = let’s look at the age column in the data frame above. The first row means that the coefficient for age (which, if you remember, was -0.9099) has to be adjusted by 0.06437573 if data point 1 is excluded. That means that the coefficient of the model without the data point would be -0.9742451 (which is -0.9099 minus 0.06437573... if the slope is negative, DFbeta values are subtracted, if it’s positive, they are added).

MOST IMPORTANT: 
Any value that changes the sign of the slope is definitely an influential point that warrants special attention... because excluding that point would change the interpretation of your results.
  - SO: eyeball the data for DFbetas and look for         values that are different by half of the absolute     value of the slope
  - Say, my slope would be 2 ... then a DFbeta value         of 1 or -1 would be alarming to me. If it’s a         slope of -4, a DFbeta value of 2 or -2 would        be alarming to me.
THEN WHAT? 
  - justify deleting datapoints ?
  
  

*Independence !!!!!!* 
Independent measures design:
(in contrary to repeated measures design, where each participant has multiple rows, e.g eye tracking data)
Each row in the data set comes from a different subject and are independent from each other:
e.g. Study 1
Subject  Sex  Voice_Pitch
1     female     233 Hz
2     female     204 Hz
3     female     242 Hz 
4      male      130 Hz
5      male      112 Hz
6      male      142 Hz

## Meeting assumptions

*Problem of multicolinarity*
1) remove corelating predictors
- think about which one is the most meaningful and drop the others
2) Consider dimension- reduction techniques such as Principal Component Analysis. These can transform several correlated variables into a smaller set of variables which you can then use as new fixed effects.


*Problem of linarity-*
1) ADD ANOTHER PREDICTOR
You might miss an important fixed effect that interacts with whatever fixed effects you already have in your model. Potentially the pattern in the residual plot goes away if this fixed effect is added.

2) TRANSFORMATION
Another (commonly chosen) option is to perform a nonlinear transformation of your response, e.g., by taking the log-transform.
You can also perform a nonlinear transformation of your fixed effects. So, if age were somehow related to pitch in a U-shaped way (perhaps, if very young people had high pitch and very old people had high pitch, too, with intermediate ages having a “dip” in pitch), then you could add age and age2 (age-squared) as predictors.

3) Finally, if you’re seeing stripes in your residual plot, then you’re most likely dealing with some kind of categorical data – and you would need to turn to a somewhat different class of models, such as logistic models.

## Linear regression
Reporting linReg
*"A linear regression analysis was used to test if the amount of sugar significantly predicted the rating of ceral products. Adjusting for the number of predictors, the results of the regression indicated that the predictor explained 57.8% of the variance in aggression metric (Adjusted R2 =.0578, F(1,74)= 103.7, p<.001). It was found that bigger amount of sugar significantly predicted the rating of product (β = -2.4614, SE =0.2417, t = -10.18, p<.001)."*

When reporting model performance: 
  mention either of R squared and corresponding explained variance
     if you have just one predictor - R squared is fine
     if yoy have more - adjusted R squared is better
  F(number of predictors, remaining degrees of freedom) = F statistic
  p-value associated with F statistic
    
When reporting results for research question (do so for every predictor):
  beta value
  standard error
  t value
  associated p-value

*Formula for making a linear model*
```{R}  
#If you have more predictors, simply use "+" (then it becomes multilevel though)
model <- lm(outcome ~ predictor, data = mydata)

summary(model)
```
  
*Visualizing the linReg*
```{r}
# The smart method
ggplot(df, aes(Rating, Sugars))+
  geom_point()+
  geom_smooth(method = lm)

# The manual method
fit1 <- lm(Rating ~ Sugars, df)

int <- fit1$coefficients[1]
slope <- fit1$coefficients[2]


ggplot(df, aes(Rating, Sugars))+
  geom_point()+
  geom_abline(intercept = int, slope = slope)
  
```




### Interpreting linReg

### Reporting LinReg
Example:
```{r}
#make a linear model
m <- lm(Aggression ~ Television, child)

#see results of fitting the model
summary(m)
```
OUTPUT:

    Coefficients:
                  Estimate Std. Error t value Pr(>|t|)    
    (Intercept) -0.0005046  0.0122778  -0.041    0.967    
    Television   0.1634249  0.0395068   4.137 3.98e-05 ***

    Signif. codes:  0 ‘***’ 0.001 ‘**’ 0.01 ‘*’ 0.05 ‘.’ 0.1 ‘ ’ 1

    Residual standard error: 0.3156 on 664 degrees of freedom
    Multiple R-squared:  0.02512,	Adjusted R-squared:  0.02365 
    F-statistic: 17.11 on 1 and 664 DF,  p-value: 3.977e-05

*Reporting simple regression*
By looking at the output above, we could report the following result:

"A linear regression analysis was used to test if the amount of television time significantly predicted children's ratings of aggression. Adjusting for the number of predictors, the results of the regression indicated that the predictor explained 2.36% of the variance in aggression metric (Adjusted R2 =.0236, F(1,664)= 17.11, p<.001). It was found that bigger amount of television significantly predicted aggressive tendencies (β = 0.1634, SE =0.0395, t = 4.137, p<.001)."

## Multilevel regression

Recap basics
    Add more predictors with a '+' sign
```{r}
summary(lm(Rating ~ Sugars + Sodium, df))
```

DO NOT USE PREDICTORS THAT CORRELATES TOGETHER (use cor tests):
```{r}
round(cor(df),2)

cor(df)

# Call forth those data points that have a correlation above .4 and below .7 REMEMBER that everything above around .7 is worrysome and should not be included in the same model (too much correlation)
which(data.frame(cor(child)) > 0.4 & data.frame(cor(child) < 1))

# a heatmap: Shows correlation visually
library(reshape2)
qplot(x=Var1, y=Var2, 
      data=melt(cor(df, use="p")), 
      fill=value, 
      geom="tile") +
   scale_fill_gradient2(limits=c(-1, 1))
```

### Interpreting

### Reporting


## Mixed effects models
*fixed effects & random effects* 

lmer= forventer at data er normal fordelt
glmer = kan man specificere hvordan ens data ser ud (hvis ens data ikke er normalfordelt)

EXAMPLE
```{r}
#packages required:
pacman::p_load(lme4, tidyverse)

#example:
politeness= read.csv("http://www.bodowinter.com/tutorial/politeness_data.csv")

#check data:
#The difference in politeness level is represented in the column called “attitude”. In that column, “pol” stands for polite and “inf” for informal. Sex is represented as “F” and “M” in the column “gender”. The dependent measure is “frequency”, which is the voice pitch measured in Hertz (Hz). To remind you, higher values mean higher pitch.

#Let’s look at the relationship between politeness and pitch by means of a boxplot:
boxplot(frequency ~ attitude*gender,
          col=c("white","lightgray"),politeness)

#What do we see? In both cases, the median line (the thick line in the middle of the boxplot) is lower for the polite than for the informal condition. However, there may be a bit more overlap between the two politeness categories for males than for females.

#making the model
politeness.model = lmer(frequency ~ attitude +
          gender + (1|subject) +
          (1|scenario), data=politeness)
summary(politeness.model)
#comments: 

#Random effects:
# Groups   Name        Variance Std.Dev.#commenting SD (below)
# scenario (Intercept) 219.5    14.81 #less variance than subjects
# subject  (Intercept) 615.6    24.81 #more subject variance than pr trial^
# Residual             645.9    25.41 # this is just the rest of epsilon
#Number of obs: 83, groups:  scenario, 7; subject, 6


#Fixed effects:
#            Estimate Std. Error t value #t value is just "estimate"/SD
#(Intercept)  256.846     16.116  15.938
#attitudepol  -19.721      5.584  -3.532
#genderM     -108.516     21.013  -5.164
#when going from "informal" to "polite", the pitch hz goes down with -19 hz
#when going from "female" to "male"  the pitch hz goes down with -108 hz


#no p-values øv ://
```

### Interpreting
... also with an interaction effect

Output:
    Formula: f0mn ~ gender * attitude + (1 | subject) + (1 | scenario)

    Fixed effects:
                        Estimate Std. Error       df t value Pr(>|t|)    
    (Intercept)          254.193      9.654   19.823  26.330  < 2e-16 ***
    genderM             -117.983     13.633   16.640  -8.654 1.46e-07 ***
    attitudepol          -15.646      5.196  200.000  -3.011  0.00294 ** 
    genderM:attitudepol    5.781      7.856  200.000   0.736  0.46263    

The interaction term in the model says that Korean men's pitch is 5.885hz higher than women's (when adjusted for gender difference) meaning that men has a higher pitch when in a polite situation than women.

*Explainer for myself*: 
Pitch for kvinder når informal er gennemsnitligt 252.895 hz
Pitch for mænd når informal er gennemsnitligt (252.895 - 112.054) = 140.841 hz
Pitch for kvinder når polite, er: 252.895 -14.568. 
Pitch for mænd når polite 252.895 - (- 112.054 - 14.568), = -127.622
Interaction betyder, at attitude polite er 5.855 mindre for mænd end for kvinder. Dvs. at mænd taler med "5 hz" mindre end kvinder (korrigeret for køn) når de taler er i en polite situation.

### Reporting

Output: 
Formula: Reaction ~ Days + (1 + Days | Subject)
Fixed effects:
            Estimate Std. Error      df t value Pr(>|t|)    
(Intercept)  251.405      6.632  18.001  37.907  < 2e-16 ***
Days          10.467      1.502  18.000   6.968 1.65e-06 ***

*Reporting*
  “We used R (R Core Team, 2019) and lmerTest (Kuznetsova, Brockhoff and Christensen, 2017) to perform a linear mixed effects analysis of the relationship between sleep deprivation and reaction time. As fixed effects, we entered the number of days subjects have been lacking sleep into the model. As random effects, we had intercepts for subjects, as well as by-subject random slopes for the effect of the number of days. The model was built using the following syntax:
 
 Reaction ~ Days + (1 + Days | Subject) 
 
 Both fixed and random effects accounted for roughly 80% of variance in the reaction time variable. Visual inspection of residual plots did not reveal any obvious deviations from homoscedasticity or normality. Reaction time has been found to significantly be modulated by number of days of sleep deprivation, β = 10.467, SE = 1.502, t = 6.968, p < .001”

## Logistic regression
Quick recap on what it is and what it does:
- Logistic regression is for categorical (binary) outcome variables
- Predicts probability of a certain kind of y given values of x
- Same output as linear model, but estimates are in the log-odd scale
- Very common classification algorithm in machine learning

We will try to predict shape (jagged vs curved) by the type of consonant using GLM and interpret the output.      
```{r}
#make a GLM model for shape
m <- glm(shape ~ consonant, 
         data = better_bobo, #  (use your data name here) 
         family = binomial)
summary(m)
```

So how do we interpret this?
*intercept: the baseline log-odds of positive class (in this case: "jagged" - last in alphabet) --> log-odds for jagged when "Bouba". It's negative to not super likely*
*consonantK (slope): Positive and significant --> Kiki makes it more likely to be jagged*

Turning Logistic regression into percentage:
To get normal probabilities out of log odds, we will need package boot and its function for inverse logit:

```{r}
#see summary again to see log odds
summary(m)

#log odds into probability for estimate of intercept (the probability of jagged shape given letter B)
boot::inv.logit(-1.5882)

#log odds into probability for estimate of going from letter B to letter K
boot::inv.logit(-1.5882 + 2.5576)

levels(better_bobo$consonant)
levels(better_bobo$shape)
```

Inverse logit of intercept is equal to 0.1696373, meaning that it's ca 17% chance that it's a jagged shape, given consonant B. By adding the beta estimate of consonantK to initial log odds - we get log odds for jagged shape, given consonant K. Inverse logit of these log odds is equal to 0.7249999, meaning it's roughly 72.5% chance that it's a jagged shape given consonant K. 

The significant p-value of beta estimate for consonant K suggests that change of consonant significantly affects the choice of shape, making choice of jagged shape way more likely in the case of consonant K.

### Assumptions of logReg
Logistic regression *does not* make many of the key assumptions of linear regression and general linear models that are based on ordinary least squares algorithms:

      - logistic regression does not require a linear relationship between the dependent and independent variables.  
      - the error terms (residuals) do not need to be normally distributed.
      - homoscedasticity is not required

However, some other assumptions still apply and you should always consider this when making your own logistic regression models!

  - binary logistic regression requires the *dependent variable to be binary*, i.e. a factor with *two levels*
      
  - logistic regression requires the observations to be independent of each other.  In other words, the observations should not come from repeated measurements or matched data - *UNLESS YOU MAKE MIXED EFFECT MODELS*
      
  - logistic regression requires there to be little or no multicollinearity among the independent variables.  This means that the independent variables should not be too highly correlated with each other (See Part 2 for multicollinearity check)
      
  - logistic regression assumes *linearity of independent variables* and log odds (otherwise it will be a bad fit)
    
  - logistic regression typically requires a large sample size.  A general guideline is that you need at minimum of 10 cases with the least frequent outcome for each independent variable in your model. For example, if you have 5 independent variables and the expected probability of your least frequent outcome is .10, then you would need a minimum sample size of 500 (10*5 / .10).

### Interpreting
Output
    
    Call:
    glm(formula = shape ~ consonant, family = binomial, data = better_bobo)
    
    Coefficients:
                Estimate Std. Error z value Pr(>|z|)    
    (Intercept)  -1.5882     0.1126  -14.11   <2e-16 ***
    consonantK    2.5576     0.1471   17.39   <2e-16 ***

    Signif. codes:  0 ‘***’ 0.001 ‘**’ 0.01 ‘*’ 0.05 ‘.’ 0.1 ‘ ’ 1

    (Dispersion parameter for binomial family taken to be 1)

        Null deviance: 1540.2  on 1119  degrees of freedom
    Residual deviance: 1168.7  on 1118  degrees of freedom
    AIC: 1172.7

*intercept: the baseline log-odds of positive class (in this case: "jagged" - last in alphabet) --> log-odds for jagged when "Bouba". It's negative to not super likely*
*consonantK (slope): Positive and significant --> Kiki makes it more likely to be jagged*

Our predictor is also a categorical variable - it also has levels
    - based on the order levels are encoded in the predictor, the very first one will be treated as the base level
    - knowing the base level is important for model interpretation: intercept shows **log odds of outcome's level 1** at **the base level of predictor**
    - all other model estimates show change in the log odds due to unit change in predictor

On the example of our data: levels(better_bobo$consonant) are "B" "K" - meaning B is base level. Therefore, the model's estimate of intercept will show log odds of jagged shape in the case of consonant B. And the beta estimate of predictor consonant K will show change in log odds for jagged shape due to change from consonant B to consonant K

### Reporting


# COMPARING MODELS
*Keywords: ANOVA, aov, BIC, Akaike*

## R-squared
See *adjusted r squared* of the models, the highest is best: R-squared tells us how much variance is explained by the model (in %)
```{r}
# Making models
fit2 <- lm(Rating~Sugars,df)
fit3 <- lm(Rating~Sugars + Sodium,df)
fit4 <- lm(Rating~Sugars + Sodium + Fiber,df)

#Looking only at r^2
summary(fit2)$adj.r.squared
summary(fit3)$adj.r.squared
summary(fit4)$adj.r.squared

# Another way to get  r-squared (look at conditional)
MuMIn::r.squaredGLMM(model2) #R2c = 0.6967788
MuMIn::r.squaredGLMM(model3) #R2c = 0.7899229
```

## Anova test
  - It will take the model objects as arguments, and return an ANOVA testing whether the more complex model is significantly better at capturing the data than the simpler model. If the resulting p-value is sufficiently low (usually less than 0.05), we conclude that the more complex model is significantly better than the simpler model, and thus favor the more complex model. If the p-value is not sufficiently low (usually greater than 0.05), we should favor the simpler model. 
```{r}
anova(fit2, fit3, fit4)
```

Reporting ANOVA:
    Output:
    Fit: aov(formula = happiness ~ country, data = happy_low)

    $country
                          diff        lwr         upr     p adj
    Finland-Denmark -0.1430671 -0.2248112 -0.06132291 0.0001234
    Syria-Denmark   -4.4564207 -4.5381649 -4.37467656 0.0000000
    Syria-Finland   -4.3133536 -4.3950978 -4.23160949 0.0000000
  **“There was a significant effect of the country on happiness of the population F(2, 2997) = 10112, p < .001. The Tukey’s HSD correction post hoc tests revealed that the happiness of population was significantly lower in Syria than in both Denmark and Finland (both p < 0.001). The happiness levels in Denmark and Finland were also found to be significantly different but to a lesser extent than compared to each country's comparison to Syria (p < 0.05)**

## aov
When we have more than three groups we want to compare with each other, we perform ANalysis Of VAriance (ANOVA). 
*compare means, when we have more than two*

The null hypothesis for ANOVE is that the mean values from all groups are the same. The alternative hypothesis is that at least one of the groups is different.

To run an ANOVA you should use function aov() and use the summary function to see the result.

NO difference in aov  (i.e. Null hypothesis is true).
Differnce in aov (i.e. Null hypothesis is NOT true)

Reporting:
**“There was no significant effect of the country on happiness, F(2, 2997) = 0.013, p > .05.”**  

You need to specify:
    F(degrees of freedom for predictor, degrees of freedom for residuals) = F value AND p value for the predictor.
    
This output however, *does not* show:
  1) whether all of the countries are different from each other, or whether it's just one country that is different from two others, 
  2) the size or direction of the differencce
  
  
```{r}
#running anova and storing the output
anova_model_nodiff <- aov(happiness ~ country, data = happy_allhigh)

#looking at the output
summary(anova_model_nodiff)


#visualize the means (more than 3) compared
#Box plot
ggplot(happy_allhigh, aes(x = country, y = happiness, colour = country)) +
  geom_boxplot(width = 0.5) +
  ggtitle("Happiness in happiest countries")+
  stat_summary(fun.y = mean, geom = "point", shape = 23, colour = "Black")

#Box plot
ggplot(happy_low, aes(x = country, y = happiness, colour = country)) +
  geom_boxplot(width = 0.5) +
  ggtitle("Happiness two happy countries and one sad")+
  stat_summary(fun.y = mean, geom = "point", shape = 23, colour = "Black")
```


## AIC and BIC
Further assesment of goodness of fit: BIC and Akaike
A *lower AIC or BIC value* indicates a *better* model. You can use either AIC or BIC when you select your model, you don't have to use both.

The *null hypothesis for ANOVA is* that the mean values from all groups are the same. The alternative hypothesis is that *at least one* of the groups is different.
```{r}
AIC(fit2, fit3, fit4) #lower is better

BIC(fit2, fit3, fit4) #lower is better
```

## Sigma
Residual standard deviation of models: Also a parameter with which we can compare models
```{r}
ResStdDev_all <- tibble(sigma_model1 = sigma(model1),
                        sigma_model2=sigma(model2),
                        sigma_model3=sigma(model3),
                        sigma_model4=sigma(model4))
```


# MACHINE LEARNING
Machine learning is an umbrella term for many different tools. These tools can largely be divded into 
*supervised methods* (k-nearest neighbour)
*unsupervised methods* (neural networks)

## Test and training sets
```{r}
#another way to create test and train set
set.seed(1)
samp <- sample(1:25000,25000*0.75) #Take 75% of data = 18750 values between 1 - 25000 (the df which we want to partition has 25000 rows in all).

training2 <- dat[samp,] # take 75% of the df 'dat' and put into train
test2 <- dat[-samp,] # take 25% and put into test.

ls.str(test2)
ls.str(training2)
```


## Classification
Classification is probably the most common data mining task
*Examples*
    Banking: determine whether a mortgage application is good
    Education: place a student into a particular track with respect to special needs
Classification is a *supervised method* and includes two or more classes for the categorical target variable
*K-nearest neighbour* is a common classification method.

## K-nearest neighbour (classification)
k-Nearest Neighbor is used most often for classification, although it is also applicable to estimation and prediction tasks.
*!* Continuous data values should be normalized using Min- Max Normalization or Z-Score Standardization so that the output remains unbiased
    Using min-max: Values lie in the range [0, 1]
    Using Z-Score Standardization, most values are typically contained in the range [-3, 3]
*Beware* that different normalization methods may results in different classifications!

k-Nearest neighbor is an example of instance-based learning, in which the training data set is stored, so that a classification for a new unclassified record may be found simply by comparing it to the most similar records in the training set.

KNN is a _non-parametric model_ which means that it does not make any assumptions about the data set

```{r}
library(class) #class is the package used to run the KNN algorithm

new <- c(0.05,0.25)
A <- c(0.0467, 0.2471)
B <- c(0.0533, 0.1912)
C <- c(0.0917, 0.2794)

data <- rbind(A, B, C)

dimnames(data) <- list(c("Dark", "Medium", "Medium"), 
                       c("Age (MMN)", "Na/K (MMN)")) # Declare true classifications of A, B, and C 
trueclass <- c("Dark", "Medium", "Medium")

```

Running k-Nearest neighbour (KNN)
    One of the ways to find the optimal K value is to calculate the square root of the total number of observations in the data set. This square root will give you the ‘K’ value.
```{r}
# find optimal k value
NROW(train_dataset) 
# say that length was 700, then we say sqrt(700) = 26.45; k = 26

knn(data, new, cl = trueclass, k = 3, prob = TRUE)

#another example:
knn.26 <- knn(train=train.loan, test=test.loan, cl=train.loan_labels, k=26) # cl = the true values 


```

Model evaluation
```{r}
#Calculate the proportion of correct classification for k = 26
ACC.26 <- 100 * sum(test.loan_labels == knn.26)/NROW(test.loan_labels)
```

check the predicted outcome against the actual value in tabular form:
```{r}
# Check prediction against actual value in tabular form for k=26
table(knn.26 ,test.loan_labels)
```

We can also make a confusion matrix, to check accuracy of model
```{r}
library(caret)
confusionMatrix(table(knn.26 ,test.loan_labels))
```


Calculate Euclidian distance (optimal)
     It just simply means the distance between two points in a plane
```{r}
install.packages("fields") library(fields)
together <- rbind(new, data) # The top row of rdist are the # distance values from New rdist(together)

# Stretch the axes
ds_newA <- sqrt((new[1] -A[1])^2 + (3*(new[2]-A[2]))^2) 
ds_newB <- sqrt((new[1] -B[1])^2 + (3*(new[2]-B[2]))^2) 
ds_newC <- sqrt((new[1] -C[1])^2 + (3*(new[2]-C[2]))^2)

```


## Neural networks

Turning categorical variables into *flag variables* (necessary for the NN to function)
A flag variable is kindda like making variable to as.factor, but you need to explicitly tell R that variables are factors.
```{r}
# Turning the categorical variable 'hotel' into a flag variable
hotel <- data.frame(Reduce(cbind, 
     lapply(levels(jürgens_df$hotel), function(x){(jürgens_df$hotel == x)*1})
))
names(hotel) = levels(jürgens_df$hotel)

# Meal
meal = data.frame(Reduce(cbind, 
     lapply(levels(jürgens_df$meal), function(x){(jürgens_df$meal == x)*1})
))
names(meal) = levels(jürgens_df$meal)

# Adding flag var's to df
jürgens_df <- cbind(jürgens_df, hotel,meal) #syntax: cbind(your_df, your_flagged_variable)

#removing variables which were made into flag-variables
rm(hotel,meal)

#REMEMBER to min/max normalize flag variables (again, for the NN to function properly). Also min/max all other numerical variables.
# min/max normalizing
for (i in c(2:43)) {
  minjürgens_df = min(jürgens_df[,i])
  maxjürgens_df = max(jürgens_df[,i])
  jürgens_df[,i] = (jürgens_df[,i] - minjürgens_df)/(maxjürgens_df-minjürgens_df)
}
```

Making test/training data set 
```{r}
# Making a test data set (30%test,70%training)
set.seed(1)
dt = sort(sample(nrow(jürgens_df), nrow(jürgens_df)*.7))

train<-jürgens_df[dt,]
test<-jürgens_df[-dt,]

```

Running neural network
```{R}
# neural net of all numerical values in df (without flagged variables)
nn1 <- nnet(train$is_cancelled~., data = train, size=10) # training the neural network on my training set: I ask it to predict the categorical variable 'is_cancelled' based on all the predictors (.) in the df.

est_isCancelled <- predict(nn1, test, type = 'class') # Checking how the neural network does at predicting values from the test df

tabNn1 = table(test$is_cancelled,est_isCancelled)

tabNn1

# Calculating the performance of the neural network:

# Accuracy
(20726+8150)/35817 # = 80.6% accuracy

# Sensitivity
8150/(5128+8150) # = 0.61

# Specificity
20726/(20726+1813) # = 0.92
```

*IMPORTANT*
To assess which variable had the greatest impact on the outcome variable (in this case 'is_cancelled') we can run a _sensitivity analysis_.
```{r}
source('https://gist.github.com/fawda123/6860630/raw/e50fc6ef30b8269660b4e65aeec7ce02beb9b551/lek_fun.r') #sort of like referring to a package

#Sensitivity analysis on all variables
all_variables <- lek.fun(nn1)

all_variables
```


## Clustering

```{R}
library(cluster)

#data
data <- c(1, 2, 3, 5, 8, 13, 21, 34, 55, 89)

# *Single-Linkage* Clustering (agnes = agglomorative clustering)
agnes_single <- agnes(data, diss = FALSE, stand = FALSE, method = "single")

# Make and plot the dendrogram. This is a general tree structure, and is gonna show how the clusters form
dend_agnes_single <- as.dendrogram(agnes_single) 

plot(dend_agnes_single,
xlab = "Index of Data Points",
ylab = "Steps",
main = "Single-Linkage Clustering")
text(1:10, 1, labels = data) #adding the labels to the clusters
axis(1, at = 1:length(data), labels = data) # also add labels, but in a smarter way

#### another way of doing single linkage clustering
data_dist <- dist(data)
cluster <- hclust(data_dist, method="single") # change "complete" to "single" to change method
plot(cluster, labels = data)

rect.hclust(cluster,
  k = 3, # k is used to specify the number of clusters
  border = "blue"
)

# Complete-Linkage Clustering
agnes_complete <- agnes(data, diss = FALSE,
stand = FALSE,
method = "complete")
# Make and plot the dendrogram dend_agn_complete <-
dend_agnes_complete <- as.dendrogram(agnes_complete) 

plot(dend_agnes_complete,
xlab = "Index of Data Points",
ylab = "Steps",
main = "Complete-Linkage Clustering")
text(1:10, 1, labels = data) #adding the labels to the clusters
axis(1, at = 1:length(data), labels = data) # also add labels, but in a smarter way

```


# COOL TOOLS (BI)

## Cross validation

## Decision trees

```{r}
library(pacman)
p_load(rpart,rpart.plot,C50,tidyverse)
```

**Standardize numeric variables** ; z-standarizing
```{r}
dat$education.num.z <- (dat$education.num - mean(dat$education.num))/sd(dat$education.num) 
```

*Use predictors to classify whether someones income is > or < $50k*
Using CART decision tree
```{r}
# For the CART decision tree, each decision node has to be binary
cartfit <- rpart(income ~ age.z + education.num.z + capital.gain.z + capital.loss.z + hours.per.week.z + race + sex + workclass + marital.status, data = dat, method = "class") #After each line, the percentage of chance going 'left or right' is shown in parenthesis. 
```
Plotting decision tree
```{r}
rpart.plot(cartfit)
```

##########################

```{r}
cr <- read.csv("data/CreditRiskTraining01(1).csv")
cr

library(rpart)
mytreeCR <- rpart(CreditRisk~Savings+Assets+Income, data=cr, method="class", control=rpart.control(minsplit=1))
mytreeCR_uninteresting <- rpart(CreditRisk~Savings+Assets+Income, data=cr, method="class")

mytreeCR

plot(mytreeCR)
plot(mytreeCR_uninteresting)

text(mytreeCR, use.n = TRUE, all = TRUE, pretty = 0, xpd = TRUE, pos=1, offset=0.9) #out of the eight at the top, 5 are good and three are bad. 6 go to the right side and 1 is bad and 5 is good. The "good" or "bad" refers to the majority of the classifications.


```
Understanding the text output of the decision tree (NEEDS FURTHER INVES)
2) If assets = low, you go down the left side. If there are non you terminate.
3) if savings = high, we go down another branch

```{r}
pacman::p_load(rattle, rpart.plot, RColorBrewer, rpart)
fancyRpartPlot(mytreeCR, caption = NULL)
```


## Social network analysis (SNA)

CREATING AN ADJACENCY MATRIX
```{r}
adjm1 <- matrix(c(0), nrow=5, ncol=5) # Creating an empty matrix without names and values.

adjm1

rownames(adjm1) <- c("A","B","C","D","E")
colnames(adjm1) <- c("A","B","C","D","E")

adjm1

# Assigning values to the entries of the matrix
adjm1[1,2] <- 1
adjm1[2,1] <- 1
adjm1[2,3] <- 1
adjm1[3,2] <- 1
adjm1[2,4] <- 1
adjm1[4,2] <- 1
adjm1[5,2] <- 1
adjm1[2,5] <- 1
adjm1[3,4] <- 1
adjm1[4,3] <- 1
adjm1[5,4] <- 1
adjm1[4,5] <- 1

adjm1
```

Create rows - then combine rows
```{r}
S1 <- c(0,0,1,0,1,0)
S2 <- c(0,0,1,1,1,0)
S3 <- c(0,0,1,1,1,0)
S4 <- c(0,0,1,1,1,0)
S5 <- c(0,0,1,1,1,0)
S6 <- c(0,0,1,1,1,0)

matrix <- rbind(S1,S2,S3,S4,S5,S6)
colnames(matrix) <- c("S1","S2","S3","S4","S5", "S6")
matrix
```

Getting started for real
```{r}
install.packages("igraph")
library(igraph)

help("graph_from_adjacency_matrix") # creates graph from an adjacency matrix

g1 <- graph_from_adjacency_matrix(adjm1, mode=c("undirected"))
plot(g1)

```

## Example A fully connected network
New adjacency matrix - a fully connected node. Every node is connected to any other nodes. Therefore, it is a star shape

```{r}

adjm2 <- matrix(c(1),nrow=5,ncol=5) # matrix where the values are all 1's
clet5=c("A","B","C","D","E")
rownames(adjm2) <- clet5
colnames(adjm2) = clet5
adjm2

# Getting rid of the 1's in the diagonal
for (i in 1:5) {
  adjm2[i,i]=0
  
}
adjm2

g2 <- graph_from_adjacency_matrix(adjm2, mode=c("undirected"))
plot(g2)

```


## Degree: The number of adjacent edges
```{r}
help(degree)
d1 <- degree(g1)
d1
d2 <-  degree(g2)
d2

adjm1
adjm2

# degree on adjm2 
# adj matrix 1 
```

## Closeness: centrality and betweenness
```{r}
help(betweenness)

# Closeness
c1 <- closeness(g1)
c2 <- closeness(g2)

c1
c2

# Betweenness
b1 <- betweenness(g1)
b2 <- betweenness(g2)  

b1 # A is not in between anyone else, therefore 0. But B has the highest betweenness-centrality (3.5) - it is crucial to connect several points. 

b2 # betweenness are all zero, because ALL nodes are adjacent( connected to all other nodes). No nodes would need to walk through any other nodes to get to a third node. 

```


## Web scraping
Scrape data from practically any website. 
Use the plugin 'SelectorGadget' in Chrome: Insert the CSS values you find via the plugins in the below code
```{r}
library(rvest)

url <- 'https://www.imdb.com/search/title/?count=100&release_date=2017,2017&title_type=feature'

webpage <- read_html(url)

#scraping rank
rank_data_html <- html_nodes(webpage, '.text-primary')
rank_data <- html_text(rank_data_html)
rank_data <- as.numeric(rank_data)

#titles
title_data_html <- html_nodes(webpage, '.lister-item-header a')
title_data <- html_text(title_data_html)

# Clean variables
title_data <- gsub("\n","",title_data) #gsub removes \n and inserts "" instead

# Combine values in a df
df <- tibble(rank_data, title_data)

```






